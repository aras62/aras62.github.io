<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

	<meta name="description" content="This is the web page for Amir Rasouli,
	 PhD candidate in Electrical Engineering and Computer Science">
    <meta name="author" content="Amir Rasouli">

    <!-- This title will show up at the top of the browser window, not in the Web page itself -->
    <title>Amir Rasouli, Ph.D.</title>

    <!-- favicons: these show at the top of your browser tab, etc. Visit http://www.favicon-generator.org/ to generate your own favicons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

    <!-- This is the jQuery plugin (necessary for Bootstrap's JavaScript plugins) -->
   <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

   <!-- Include all compiled plugins (below), or include individual files as needed -->
   <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

    <!-- These are the includes for Bootstrap CSS and JS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

    <!-- This is the include for the FontAwesome fonts -->
    <link rel="stylesheet" href="css/font-awesome.min.css">

    <!-- This is the include for the Academicons fonts -->
    <link rel="stylesheet" href="css/academicons.min.css">

    <!-- This is the include for your own personal style sheet -->
    <link href="css/custom.css" rel="stylesheet">

    <!-- favicons  -->
    <link rel="apple-touch-icon" sizes="57x57" href="favicons/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="favicons/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="favicons/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="favicons/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="favicons/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="favicons/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="favicons/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="favicons/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="favicons/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192" href="favicons/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicons//favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="favicons//favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicons//favicon-16x16.png">


</head>

<body>

	<!-- Fixed navbar -->
	<nav class="navbar navbar-default navbar-fixed-top" style="background-color:#057088" >
         <div class="container">
          <div class="navbar-header">
             <button type="button" class="navbar-toggle collapsed"
			 data-toggle="collapse" data-target="#navbar"
			 aria-expanded="false" aria-controls="navbar">
               <span class="sr-only">Toggle navigation</span>
               <span class="icon-bar"></span>
               <span class="icon-bar"></span>
               <span class="icon-bar"></span>
             </button>
             <h1 id="brand-name"><a class="navbar-brand"
				  href="index.html">Amir Rasouli</a></h1>
          </div>
          <div id="navbar" class="navbar-collapse collapse">
			  <ul class="nav navbar-nav pull-right">
              <li id="about" ><a href="index.html">About Me</a></li>
  			  <li id="research" class="active"><a href="research.html">Research</a></li>
              <li id="publications"><a href="publications.html">Publications</a></li>
  			  <li id="affiliations"><a href="affiliations.html">Affiliations</a></li>
  			  <li id="cv"><a href="cv.html">CV</a></li>
              </ul>
          </div><!--/.nav-collapse -->
         </div>
       </nav>

    <div class="container main-content">
      <div class="row">
         <div class="col-sm-12">
         <h2 class="page-header">Research</h2>
         </div>
      </div>


<div class="row">

			<h3 >Main Research</h3>
			<dd>&nbsp</dd><dd>&nbsp</dd>

			<div style="padding:0 33px; float:left">
				<dd>&nbsp</dd>
				 <img  src="img/pie_trajectory.png"  alt="trajectory_prediction" width = "400" >
			</div>
			<h4 class="office-hours">Pedestrian Trajectory Prediction</h4>
			<dl class="dl">
				 <dd> An intelligent system should be able to understand the intentions or underlying motives of pedestrians and to predict their forthcoming actions. To date, only a few public datasets were proposed for the purpose of studying pedestrian behavior prediction in the context of intelligent driving. To this end, we propose a novel large-scale dataset designed for pedestrian intention estimation (PIE). We conducted a large-scale human experiment to establish human reference data for pedestrian intention in traffic scenes. We propose models for estimating pedestrian crossing intention and predicting their future trajectory..<br>
					 [<a href=https://github.com/aras62/PIEPredict style="color:#0066ff ! important;">code</a>]
					 [<a href=http://data.nvision2.eecs.yorku.ca/PIE_dataset/ style="color:#0066ff ! important;">dataset</a>]
					  [<a href=http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf style="color:#0066ff ! important;">paper</a>]
				   </dd>
			</dl>


			<div style="padding:0 33px; float:left">
				<dd>&nbsp</dd>
				 <img  src="img/bmvc_diagram.png"  alt="action_anticipation" width = "400" >
			</div>
			<h4 class="office-hours">Pedestrian Action Anticipation</h4>
			<dl class="dl">
				 <dd> One of the major challenges for autonomous vehicles in urban environments is to
					understand and predict other road usersâ€™ actions, in particular, pedestrians at the point of crossing.
					We propose a solution for the problem of pedestrian action anticipation using a novel stacked RNN
					architecture in which information collected from various sources, both scene dynamics and visual
					features, is gradually fused into the network at different levels of processing. We investigate
					the impact of the length of observation, time to event, types of features and fusion strategies on the performance of
					 the proposed method.<br>
					 [<a href=https://github.com/aras62/SF-GRU style="color:#0066ff ! important;">code</a>]
					 [<a href=http://data.nvision2.eecs.yorku.ca/PIE_dataset/ style="color:#0066ff ! important;">dataset</a>]
					  [<a href=https://bmvc2019.org/wp-content/uploads/papers/0283-paper.pdf style="color:#0066ff ! important;">paper</a>]
				   </dd>
			</dl>

			<dd>&nbsp</dd><dd>&nbsp</dd><dd>&nbsp</dd>

			<div style="padding:0 53px; float:left">
				<dd>&nbsp</dd>
			   <div class="row" >
				   <img  src="img/gifs/looking.gif" alt="looking" width = "97" align="left">
				   <img  src="img/gifs/nod1.gif" alt="nod" width = "97" align="left">
				   <img  src="img/gifs/wave1.gif" alt="handwave" width = "97"  align="left">
				   <img  src="img/gifs/wave2.gif"  alt="handwave" width = "97" align="left">
				</div>
				<div class="row">
				   <img  src="img/gifs/wave4.gif" alt="handwave" width = "97" align="left" >
				   <img  src="img/gifs/nod2.gif" alt="nod" width = "97"  align="left">
				   <img  src="img/gifs/wave3.gif" alt="handwave" width = "97"  align="left">
				   <img  src="img/gifs/nod3.gif"  alt="nod" width = "97" align="left">
			   </div>
		    </div> <!-- div comm-->

               <h4 class="office-hours">How Pedestrians Interact with Traffic</h4>
               <dl class="dl">
				   <dd> Understanding road users behavior, in particular, pedestrians is a daunting task. Pedestrian decision-making process
					   can be influenced by various factors	 including road conditions, pedestrian's characteristics, environmental conditions,
						social forces, etc. As part of my research, I am interested to identify what
					   	impacts a pedestrian to make a crossing decision. This includes determining what types of behavior
					   pedestrians demonstrate at the time of crossing, and how do they communicate with traffic.<br>
					   [<a href=http://data.nvision2.eecs.yorku.ca/JAAD_dataset/ style="color:#0066ff ! important;">dataset</a>]
						[<a href=https://ieeexplore.ieee.org/abstract/document/8569324 style="color:#0066ff ! important;">paper</a>]
					</dd>
               </dl>

			   <dd>&nbsp</dd><dd>&nbsp</dd>
			   <div style="padding:0 33px; float:left">
				   <dd>&nbsp</dd>
					<img  src="img/ped_det.png"  alt="pedestrian_detection" width = "400" >
			   </div>
			   <h4 class="office-hours">Challenges in Detecting Pedestrian</h4>
               <dl class="dl">
                 	<dd> Object detection and recognition in traffic scenes is challenging, particularly when it comes to identifying pedestrians.
						 Pedestrians can assume different poses, their appearances may change significantly (especially under different weather
						 conditions) and can be mistaken with objects with similar visual properties. My objective is to measure the limitations of existing object detection methods using traffic data collected under various conditions and propose
						solutions to further improve their performance.<br>
						[<a href=http://data.nvision2.eecs.yorku.ca/JAAD_dataset/ style="color:#0066ff ! important;">dataset</a>]
   					 [<a href=http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Rasouli_Its_Not_All_About_Size_On_the_Role_of_Data_ECCVW_2018_paper.pdf
					  style="color:#0066ff ! important;">paper</a>]
               </dl>

			   <dd>&nbsp</dd>
			   <dd>&nbsp</dd>
			   <div style="padding:0 0px; float:left">
				   <dd>&nbsp</dd>
				   <img  src="img/intent.png"  alt="pedestrian_intent" style="padding:0 33px; float:left" width = "470" align="right">
			   </div>
			   <h4 class="office-hours">Are They Going to Cross?</h4>
			   <dl class="dl">
                 	<dd> Using the knowledge of pedestrian behavior and ability to efficiently search and detect objects in the scene,
						 I working on addressing the problem of pedestrian intention estimation. Here, the goal is to identify the pedestrian,
						 understand their surrounding context, interpret their actions, and predict whether they are going to make a crossing decision.<br>
						  [<a href=http://data.nvision2.eecs.yorku.ca/JAAD_dataset/ style="color:#0066ff ! important;">dataset</a>]
						  [papers: <a href=https://arxiv.org/pdf/1609.04741.pdf style="color:#0066ff ! important;">arXiv</a>,
						   <a href=http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Rasouli_Are_They_Going_ICCV_2017_paper.pdf
						   style="color:#0066ff ! important;">ICCVW</a>,
						   <a href=https://ieeexplore.ieee.org/abstract/document/7995730/
						   style="color:#0066ff ! important;">IV</a>,
						   <a href=https://ieeexplore.ieee.org/document/8241847/
						   style="color:#0066ff ! important;">Trans. IV</a>]
					</dd>

			<dd>&nbsp</dd><dd>&nbsp</dd><dd>&nbsp</dd><dd>&nbsp</dd>

			<h3 >Other Research</h3>

			<dd>&nbsp</dd><dd>&nbsp</dd>

		   <div style="padding:0 0px; float:left">
			   <dd>&nbsp</dd>
			   <img  src="img/p3o3.png"  alt="saliency" style="padding:0 33px; float:left" width = "470" align="right">
		   </div>
		   <h4 class="office-hours">Do Saliency Models Detect Odd-One-Out Targets?</h4>
		   <dl class="dl">
				<dd> Despite recent successes in visual saliency prediction and near-human performance on benchmarks,
					 most of the saliency models do not properly capture some of the important properties of human visual
					 attention. We evaluate the behavior of 20 saliency models on two new datasets: artificial psychophysical
					  images and natural odd-one-out images. We show that majority of the models do not discriminate t
					  argets that differ by color, orientation and size, which are the features that strongly guide human attention.<br>
					  [<a href=https://github.com/ykotseruba/P3O3_metrics
			   		style="color:#0066ff ! important;">code</a>]
					   [<a href=http://data.nvision2.eecs.yorku.ca/P3O3/
						style="color:#0066ff ! important;">dataset</a>]
					  [<a href=https://bmvc2019.org/wp-content/uploads/papers/0203-paper.pdf
					   style="color:#0066ff ! important;">paper</a>]
				</dd>

			<dd>&nbsp</dd><dd>&nbsp</dd>

			<div style="padding:0 0px; float:left">
				<dd>&nbsp</dd>
				<img  src="img/afinder.jpg"  alt="attention_search" style="padding:0 33px; float:left" width = "470" align="right">
			</div>
			<h4 class="office-hours">Attention-based Visual Search</h4>
			<dl class="dl">
				 <dd> The proposed algorithm guides the robot towards the sought
						object using the relevant stimuli provided by the visual
						sensors. We propose a new model that actively extracts visual
						information via visual attention techniques and, in conjunction
						with a non-myopic decision-making algorithm,
						leads the robot to search more relevant areas of the environment.
						We examine the effect of visual saliency on search with respect to the type of sought objects and
						 search environments such as structured office
						environments and disaster cluttered envirnments.<br>
						[<a href=http://data.nvision2.eecs.yorku.ca/3DGEMS/
						 style="color:#0066ff ! important;">dataset</a>]
					   [<a href=https://link.springer.com/article/10.1007/s10514-019-09882-z
						style="color:#0066ff ! important;">paper</a>]
				 </dd>




			<h3 >Past Research</h3>


		 <dd>&nbsp</dd>	<dd>&nbsp</dd>

			<div style="padding:0 33px; float:left">
			<dd>&nbsp</dd>
			<video width="400" autoplay loop>

			<source src="img/search.mp4" type="video/mp4">
			</video></dd></div>
              <h4 class="office-hours">Active Visual Search</h4>
               <dl class="dl">
	                <dd> To optimize the task of visual object search in unknown environments, we incorporated visual saliency to
							identify which parts of the environment is more interesting, both in terms of standing out with respect
							to the surroundings and similarity to the object of interest. We confirmed
							the importance of using such a technique by performing extensive experiments both in highly complex simulated
							and real environments using mobile robotic platforms.<br>
							 [<a href=https://github.com/aras62/Active-visual-search style="color:#0066ff ! important;">code</a>]
							 [papers: <a href=https://ieeexplore.ieee.org/abstract/document/6816832/ style="color:#0066ff ! important;">CRV14</a>,
							 <a href=https://pdfs.semanticscholar.org/c9e6/249e156943160df54e127f5359be15169e82.pdf?_ga=2.130383439.1176215022.1535399868-1929962795.1527520915
							 style="color:#0066ff ! important;">i-SAIRAS</a>,
							 <a href=https://arxiv.org/pdf/1702.04292.pdf style="color:#0066ff ! important;">ISACS IROS</a>,
							 <a href=https://ieeexplore.ieee.org/abstract/document/7801501/ style="color:#0066ff ! important;">CRV16</a>]

	           </dl>

			   <dd>&nbsp</dd><dd>&nbsp</dd><dd>&nbsp</dd>
			   <div style="padding:0 33px; float:left">
			   	<dd>&nbsp</dd>
			   	<video width="198" autoplay loop >
			   		<source src="img/track.mp4" type="video/mp4">
			   	</video>
			   	<video width="198" autoplay loop>
			   		<source src="img/track_imp.mp4" type="video/mp4">
			   	</video>
		   	   </div>
   			   <h4 class="office-hours"></i>Object Tracking</h4>
                  <dl class="dl">
                    <dd> One of the common challenges in object tracking is the inclusion of background information in the object frame which can distract the tracker to
						the background. By replacing the rectangular-shaped frame with an active contour box that assumes the shape of the object, we minimized the background noise,
						and therefore improved the tracking process.
					</dd>
   				  </dl>
<!-- .row -->
</div>

    <!-- .container -->
	<footer class="container">
        <div class="row">

            <!-- academic icons: For more icons, please see https://jpswalsh.github.io/academicons/ -->
            <div class="col-xs-12 col-sm-3 social-icons pull-right">

               <!-- Visit Google Scholar to set up an account -->
               <a class="btn btn-primary btn-lg" href="https://scholar.google.ca/citations?user=lkBWPEsAAAAJ&hl=en"><i class="ai ai-google-scholar" title="Google Scholar"></i></a>

               <!-- Visit Researchgate.net to set up an account -->
               <a class="btn btn-primary btn-lg" href="https://www.researchgate.net/profile/Amir_Rasouli4"><i class="ai ai-researchgate" title="Research Gate"></i></a>

               <!-- Visit LinkedIn.net to set up an account  -->
               <a class="btn btn-primary btn-lg" href="https://www.linkedin.com/in/amir-rasouli-7b609747/"><i class="fa fa-linkedin" title="LinkedIn"></i></a>

               <!-- Visit Twitter to set up an account  -->
               <a class="btn btn-primary btn-lg" href="https://github.com/aras62"><i class="fa fa-github" title="GitHub"></i></a>
            </div>

        </div>
        <!-- .row -->
   </footer>
   <!-- This in the include to style code, if you put code into your website, for instructions, see: https://github.com/google/code-prettify -->
   <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
</body>

</html>
